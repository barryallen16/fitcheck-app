{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/datasets/barryallen16/indofashion-llm-classification/resolve/main/indofashion_results.jsonl"
      ],
      "metadata": {
        "id": "Ba0S2OQ8o44Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87dcf166-32aa-43af-df0e-01b9ba1042cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing indofashion_results.jsonl\n",
            "--2025-10-19 13:58:03--  https://huggingface.co/datasets/barryallen16/indofashion-llm-classification/resolve/main/indofashion_results.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.40, 13.35.202.121, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: /api/resolve-cache/datasets/barryallen16/indofashion-llm-classification/0e3267f2d8fbe6681000aa449715ab21f6ae445d/indofashion_results.jsonl?%2Fdatasets%2Fbarryallen16%2Findofashion-llm-classification%2Fresolve%2Fmain%2Findofashion_results.jsonl=&etag=%22091dfaa77bafd0b915af8b6b1840ec9b0a1d048a%22 [following]\n",
            "--2025-10-19 13:58:03--  https://huggingface.co/api/resolve-cache/datasets/barryallen16/indofashion-llm-classification/0e3267f2d8fbe6681000aa449715ab21f6ae445d/indofashion_results.jsonl?%2Fdatasets%2Fbarryallen16%2Findofashion-llm-classification%2Fresolve%2Fmain%2Findofashion_results.jsonl=&etag=%22091dfaa77bafd0b915af8b6b1840ec9b0a1d048a%22\n",
            "Reusing existing connection to huggingface.co:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16149 (16K) [text/plain]\n",
            "Saving to: â€˜indofashion_results.jsonlâ€™\n",
            "\n",
            "indofashion_results 100%[===================>]  15.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-10-19 13:58:03 (244 MB/s) - â€˜indofashion_results.jsonlâ€™ saved [16149/16149]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-wbyainIywN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.57.0\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install qwen-vl-utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF2zZZiaI2bG"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !wget https://huggingface.co/datasets/barryallen16/fitcheck-annotate-dataset/resolve/main/fitcheck-dataset.zip\n",
        "# !unzip fitcheck-dataset.zip\n",
        "%%capture\n",
        "!curl -L \"https://drive.usercontent.google.com/download?id=1MdWBJxO7wQrD00_rBDKX8NbRrxRcKgff&export=download&authuser=0&confirm=t\" -o indofashion_dataset.zip\n",
        "!unzip indofashion_dataset.zip\n",
        "!rm indofashion_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG_CWPcRI36i",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "%%capture\n",
        "import torch\n",
        "from unsloth import FastVisionModel\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import json\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Loading Qwen3-VL with Unsloth optimization...\")\n",
        "\n",
        "model, processor = FastVisionModel.from_pretrained(\n",
        "    # \"unsloth/Qwen2.5-VL-7B-Instruct-unsloth-bnb-4bit\",  # 7B model now fits in memory!\n",
        "    \"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\",\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
        ")\n",
        "\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "print(\"âœ… Model loaded with Unsloth optimization!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DNoibgvI5db",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def classify_clothing_unsloth(image_path, context=None):\n",
        "    \"\"\"\n",
        "    Classify clothing using Unsloth-optimized Qwen3-VL model\n",
        "    Returns structured JSON metadata\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file\n",
        "        context: Optional context string to guide classification (e.g., \"women's festive wear\", \"men's traditional\")\n",
        "    \"\"\"\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Detailed prompt for Indian ethnic wear\n",
        "    base_prompt = \"\"\"Analyze this clothing item image. Focus only on the garment, ignore background and people.\"\"\"\n",
        "\n",
        "    # Add context if provided\n",
        "    if context:\n",
        "        base_prompt += f\"\\n\\nContext: {context}\"\n",
        "\n",
        "    base_prompt += \"\"\"\n",
        "\n",
        "Provide detailed analysis in ONLY valid JSON format:\n",
        "\n",
        "{\n",
        "  \"specific_type\": \"detailed garment name (e.g., silk saree, embroidered women kurta, denim jeans, cotton shirt)\",\n",
        "  \"category\": \"Choose the MOST ACCURATE category from: saree/women_kurta/leggings_salwar/palazzo/lehenga/dupatta/blouse/gown/dhoti_pants/petticoats/women_mojari/women_anarkali_kurta/women_a_line_kurta/men_kurta/nehru_jacket/sherwani/men_mojari/men_pagdi/shirt/t_shirt/jeans/trousers/pants/skirt/jacket/other\",\n",
        "  \"color_primary\": \"dominant color with shade (e.g., deep maroon, light blue)\",\n",
        "  \"color_secondary\": [\"secondary color1\", \"secondary color2\"] or [] if none,\n",
        "  \"pattern\": \"design pattern: floral/paisley/solid/geometric/embroidery/prints/striped/checkered/plain\",\n",
        "  \"material\": \"fabric type: silk/cotton/chiffon/georgette/denim/linen/crepe/velvet/brocade/chanderi/banarasi/polyester/khadi\",\n",
        "  \"style\": \"traditional/contemporary/fusion/casual/formal/festive/western\",\n",
        "  \"occasions\": [\"wedding\", \"festival\", \"casual\", \"office\", \"party\", \"daily_wear\", \"formal\"],\n",
        "  \"weather\": [\"summer\", \"winter\", \"monsoon\", \"all_season\"],\n",
        "  \"formality\": \"casual/semi_formal/formal/festive\",\n",
        "  \"embellishments\": [\"embroidery\", \"sequins\", \"prints\", \"zari_work\", \"mirror_work\", \"gota_patti\", \"stone_work\", \"thread_work\"] or [\"plain\"] if no embellishments,\n",
        "  \"gender\": \"male/female/unisex\",\n",
        "  \"fit\": \"loose/fitted/flowy/structured/regular/straight\",\n",
        "  \"blouse_details\": {\n",
        "    \"visible\": true/false,\n",
        "    \"color\": \"ACTUAL blouse color if CLEARLY visible, null if not visible or unclear\",\n",
        "    \"pattern\": \"blouse pattern ONLY if clearly visible (solid/embroidered/printed/striped), null if not visible\",\n",
        "    \"sleeve_style\": \"ONLY if sleeves are clearly visible: full_sleeve/half_sleeve/sleeveless/three_quarter, null if not visible\",\n",
        "    \"neckline\": \"ONLY if neckline is clearly visible: round/v_neck/boat_neck/square/high_neck/sweetheart/halter, null if not visible\",\n",
        "    \"embellishments\": [\"embroidery\", \"sequins\", \"zari_work\"] or [\"plain\"] if no embellishments, null if not visible,\n",
        "    \"material\": \"blouse fabric if distinguishable (silk/cotton/chiffon), null if not visible\",\n",
        "    \"match_level\": \"perfect_match/complementary/contrasting, null if not visible\"\n",
        "  } or null if category is not saree or blouse not visible\n",
        "}\n",
        "\n",
        "IMPORTANT CLASSIFICATION RULES:\n",
        "- For regular shirts (button-down): use category \"shirt\"\n",
        "- For jeans: use category \"jeans\"\n",
        "- For regular trousers/pants: use category \"trousers\" or \"pants\"\n",
        "- For traditional Indian kurtas: use \"men_kurta\" or \"women_kurta\"\n",
        "- For anarkali style: use \"women_anarkali_kurta\"\n",
        "- For palazzo pants: use \"palazzo\"\n",
        "- For salwar/churidar: use \"leggings_salwar\"\n",
        "- Only use ethnic categories (saree, lehenga, sherwani, etc.) for traditional Indian wear\n",
        "- If unsure, use \"other\" rather than forcing an incorrect ethnic category\n",
        "- For embellishments: use [\"plain\"] if garment has no decorative work, use [] empty list should be avoided\n",
        "\n",
        "CRITICAL BLOUSE ANALYSIS RULES (for sarees only):\n",
        "- BE VERY CAREFUL: Only describe what you can ACTUALLY SEE in the image\n",
        "- If category is NOT \"saree\", return null for entire \"blouse_details\"\n",
        "- If category IS \"saree\" but blouse is not visible, return null for entire \"blouse_details\"\n",
        "- Set \"visible\": true ONLY if you can clearly see the blouse in the image\n",
        "- Set \"visible\": false if:\n",
        "  * Only the saree is visible (product shot without model)\n",
        "  * Blouse is covered or not in frame\n",
        "  * Image shows only draped saree without blouse\n",
        "- DO NOT assume or guess blouse details:\n",
        "  * If you cannot see the blouse color clearly, use null\n",
        "  * If sleeves are not visible or unclear, use null for \"sleeve_style\"\n",
        "  * If neckline is not visible (covered by saree pallu, not in frame, or unclear), use null for \"neckline\"\n",
        "  * If embellishments cannot be determined, use null\n",
        "- DO NOT copy saree color/pattern to blouse unless you can actually see a matching blouse\n",
        "- For \"match_level\":\n",
        "  * Only assess if blouse is clearly visible\n",
        "  * Use null if blouse is not visible or details are unclear\n",
        "\n",
        "Return ONLY the JSON object, no other text.\"\"\"\n",
        "    prompt = base_prompt\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Prepare inputs\n",
        "    text = processor.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    # Generate classification with Unsloth optimization\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.3,\n",
        "            use_cache=True  # Unsloth optimizes KV cache\n",
        "        )\n",
        "\n",
        "    # Decode output\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):]\n",
        "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    # Parse JSON\n",
        "    try:\n",
        "        json_start = output_text.find('{')\n",
        "        json_end = output_text.rfind('}') + 1\n",
        "        json_str = output_text[json_start:json_end]\n",
        "        classification = json.loads(json_str)\n",
        "\n",
        "        # Check for template responses\n",
        "        if (classification.get(\"specific_type\") == \"detailed garment name (e.g., silk saree, embroidered kurti)\" or\n",
        "            classification.get(\"category\") == \"exact category: kurta/kurti/palazzo/churidar/salwar/saree/lehenga/anarkali_suit/gown/dupatta/blouse/choli/dhoti_pants/skirt/shirt/t_shirt/jeans/trousers/crop_top/peplum_top/anarkali_top/cape/jacket/shawl/lehenga_set\"):\n",
        "            print(f\"Warning: Template response for {image_path}\")\n",
        "            return None\n",
        "\n",
        "        return classification\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Warning: Could not parse JSON for {image_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing for {image_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def process_clothing_list_unsloth(data_list, existing_jsonl, output_jsonl, context_mapper=None, start=None, end=None):\n",
        "    \"\"\"\n",
        "    Process a list of clothing items with Unsloth optimization\n",
        "    Saves results in both CSV and JSONL formats\n",
        "    Automatically resumes from where it left off\n",
        "\n",
        "    Args:\n",
        "        data_list: List of dictionaries with 'image_path' and optional context fields\n",
        "                   Example: [{'image_path': 'path.jpg', 'product_title': 'Saree', 'class_label': 'saree'}, ...]\n",
        "        output_csv: Output CSV filename\n",
        "        output_jsonl: Output JSONL filename\n",
        "        context_mapper: Optional function that takes data dict and returns context string\n",
        "                       Example: lambda data: f\"Product: {data.get('product_title')}\"\n",
        "                       If None, will use default context from data fields\n",
        "        start: Optional starting index (inclusive, 0-based). Default is 0\n",
        "        end: Optional ending index (exclusive). Default is None (process till end)\n",
        "               Example: start=10, end=100 will process items 10-99 (90 items)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    processed_paths = set()\n",
        "\n",
        "    # Set default start\n",
        "    if start is None:\n",
        "        start = 0\n",
        "\n",
        "    # Validate start and end\n",
        "    if start < 0:\n",
        "        start = 0\n",
        "    if end is not None and end > len(data_list):\n",
        "        end = len(data_list)\n",
        "    if end is not None and start >= end:\n",
        "        print(f\"âŒ Error: start ({start}) must be less than end ({end})\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Load existing results if files exist\n",
        "    if os.path.exists(existing_jsonl):\n",
        "        print(f\"ðŸ“‚ Found existing output file: {existing_jsonl}\")\n",
        "        print(f\"ðŸ”„ Loading previously processed images...\")\n",
        "        try:\n",
        "            with open(existing_jsonl, 'r') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        item = json.loads(line.strip())\n",
        "                        results.append(item)\n",
        "                        processed_paths.add(item['image_path'])\n",
        "                    except json.JSONDecodeError:\n",
        "                        continue\n",
        "            print(f\"âœ… Loaded {len(results)} previously processed images\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Error loading existing file: {e}\")\n",
        "            print(f\"ðŸ”„ Starting fresh...\")\n",
        "            results = []\n",
        "            processed_paths = set()\n",
        "\n",
        "    # Slice the data_list based on start and end\n",
        "    if end is not None:\n",
        "        selected_items = data_list[start:end]\n",
        "        print(f\"ðŸ“Œ Processing range: items {start} to {end-1} (total: {end-start} items)\")\n",
        "    else:\n",
        "        selected_items = data_list[start:]\n",
        "        print(f\"ðŸ“Œ Processing range: items {start} to end (total: {len(selected_items)} items)\")\n",
        "\n",
        "    # Filter out already processed items\n",
        "    items_to_process = [\n",
        "        item for item in selected_items\n",
        "        if item.get('image_path') and item['image_path'] not in processed_paths\n",
        "    ]\n",
        "\n",
        "    total_to_process = len(items_to_process)\n",
        "    skipped = len(selected_items) - total_to_process\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ðŸ“Š Range: {start} to {end if end else len(data_list)}\")\n",
        "    print(f\"ðŸ“Š Items in range: {len(selected_items)}\")\n",
        "    print(f\"ðŸ“Š Already processed (skipped): {skipped}\")\n",
        "    print(f\"ðŸ“Š New images to process: {total_to_process}\")\n",
        "    print(f\"ðŸ“Š Total processed so far: {len(processed_paths)}\")\n",
        "    print(f\"ðŸš€ Using Unsloth-optimized Qwen3-VL-8B\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    if total_to_process == 0:\n",
        "        print(\"âœ… All images in this range already processed!\")\n",
        "        return pd.DataFrame(results) if results else pd.DataFrame()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Process with progress bar\n",
        "    for idx, data_item in enumerate(tqdm(items_to_process, desc=\"Classifying\")):\n",
        "        try:\n",
        "            image_path = data_item.get('image_path')\n",
        "\n",
        "            # Check if image file exists\n",
        "            if not os.path.exists(image_path):\n",
        "                print(f\"\\nâš ï¸  Image not found: {image_path}\")\n",
        "                continue\n",
        "\n",
        "            # Build context using custom mapper or default\n",
        "            if context_mapper:\n",
        "                context = context_mapper(data_item)\n",
        "            else:\n",
        "                # Default context builder\n",
        "                context_parts = []\n",
        "                if 'image_path' in data_item:\n",
        "                    context_parts.append(f\"Image path: {data_item['image_path']}\")\n",
        "                if 'product_title' in data_item:\n",
        "                    context_parts.append(f\"Product Title: {data_item['product_title']}\")\n",
        "                if 'class_label' in data_item:\n",
        "                    context_parts.append(f\"Class Label: {data_item['class_label']}\")\n",
        "\n",
        "                context = \"\\n\".join(context_parts) if context_parts else None\n",
        "\n",
        "            classification = classify_clothing_unsloth(image_path, context=context)\n",
        "\n",
        "            if classification:\n",
        "                result_entry = {\n",
        "                    'filename': os.path.basename(image_path),\n",
        "                    'image_path': image_path,\n",
        "                    'classification': classification\n",
        "                }\n",
        "\n",
        "                processed_paths.add(image_path)\n",
        "\n",
        "                # Append to JSONL immediately (for crash recovery)\n",
        "                with open(output_jsonl, 'a') as f:\n",
        "                    f.write(json.dumps(result_entry) + '\\n')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ Error processing {image_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Calculate metrics\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"âœ… Processing Complete!\")\n",
        "    print(f\"âœ… Range processed: {start} to {end if end else len(data_list)}\")\n",
        "    print(f\"âœ… Total images in output: {len(results)}\")\n",
        "    print(f\"âœ… New images processed in this run: {total_to_process}\")\n",
        "    print(f\"âœ… Time taken for new images: {elapsed_time/60:.1f} minutes\")\n",
        "    if total_to_process > 0:\n",
        "        print(f\"âœ… Average: {elapsed_time/total_to_process:.2f} seconds per image\")\n",
        "    print(f\"âœ… Saved to JSONL: {output_jsonl}\")\n",
        "    print(f\"{'='*80}\\n\")"
      ],
      "metadata": {
        "id": "_GdZbiS_dV89",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "with open('val_data.json', 'r') as file:\n",
        "    for line in file:\n",
        "        try:\n",
        "            data_list.append(json.loads(line))\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "df = process_clothing_list_unsloth(\n",
        "    data_list=data_list,\n",
        "    existing_jsonl=\"indofashion_results.jsonl\",\n",
        "    output_jsonl=\"new_indofashion_results.jsonl\",\n",
        "    start=0,\n",
        "    end=30\n",
        ")"
      ],
      "metadata": {
        "id": "qImzLIwJhCoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb1ccb38-1dc1-4a96-df52-bfc2c3ffb3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Found existing output file: indofashion_results.jsonl\n",
            "ðŸ”„ Loading previously processed images...\n",
            "âœ… Loaded 26 previously processed images\n",
            "ðŸ“Œ Processing range: items 0 to 29 (total: 30 items)\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š Range: 0 to 30\n",
            "ðŸ“Š Items in range: 30\n",
            "ðŸ“Š Already processed (skipped): 26\n",
            "ðŸ“Š New images to process: 4\n",
            "ðŸ“Š Total processed so far: 26\n",
            "ðŸš€ Using Unsloth-optimized Qwen3-VL-8B\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Classifying: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:51<00:00, 27.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "âœ… Processing Complete!\n",
            "âœ… Range processed: 0 to 30\n",
            "âœ… Total images in output: 26\n",
            "âœ… New images processed in this run: 4\n",
            "âœ… Time taken for new images: 1.9 minutes\n",
            "âœ… Average: 27.95 seconds per image\n",
            "âœ… Saved to JSONL: new_indofashion_results.jsonl\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# filename = \"indofashion_results.jsonl\"\n",
        "# # Remove the file if it already exists\n",
        "# if os.path.exists(filename):\n",
        "#     os.remove(filename)\n",
        "#     print(f\"Removed existing {filename}\")"
      ],
      "metadata": {
        "id": "zhhTh8ynFHQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://huggingface.co/datasets/barryallen16/indofashion-llm-classification/resolve/main/indofashion_results.jsonl"
      ],
      "metadata": {
        "id": "oQtmbICkFH7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title\n",
        "# import json\n",
        "\n",
        "# file1_path = \"indofashion_results.jsonl\"\n",
        "# file2_path = \"new_indofashion_results.jsonl\"\n",
        "\n",
        "# # Read lines from the second file\n",
        "# lines2 = []\n",
        "# try:\n",
        "#     with open(file2_path, 'r') as f:\n",
        "#         for line in f:\n",
        "#             lines2.append(line.strip())\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Warning: File not found at {file2_path}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error reading {file2_path}: {e}\")\n",
        "\n",
        "# # Append lines from the second file to the first file\n",
        "# try:\n",
        "#     with open(file1_path, 'a') as f:\n",
        "#         for line in lines2:\n",
        "#             f.write(line + '\\n')\n",
        "#     print(f\"Successfully appended {file2_path} to {file1_path}\")\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Warning: File not found at {file1_path}. Cannot append.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error writing to {file1_path}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjnqU-P5vj8i",
        "outputId": "52da2adf-b688-405d-e11f-ab7c4e8a60a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully appended new_indofashion_results.jsonl to indofashion_results.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm new_indofashion_results.jsonl"
      ],
      "metadata": {
        "id": "_uiaFkRExjUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import HfApi\n",
        "# from google.colab import userdata\n",
        "\n",
        "# hf_token = userdata.get('WRITE_HF_TOKEN')\n",
        "# api = HfApi(token=hf_token)  # Pass the token to HfApi\n",
        "\n",
        "# repo_id = \"barryallen16/indofashion-llm-classification\"\n",
        "# filename = \"indofashion_results.jsonl\"\n",
        "# path_in_repo = filename\n",
        "\n",
        "# try:\n",
        "#     # Create the repository if it doesn't exist\n",
        "#     api.create_repo(repo_id, repo_type=\"dataset\", exist_ok=True)\n",
        "\n",
        "#     # Upload the file\n",
        "#     api.upload_file(\n",
        "#         path_or_fileobj=filename,\n",
        "#         path_in_repo=path_in_repo,\n",
        "#         repo_id=repo_id,\n",
        "#         repo_type=\"dataset\",\n",
        "#         token=hf_token,  # Pass the token here as well\n",
        "#         commit_message=f\"Update {filename}\"\n",
        "#     )\n",
        "#     print(f\"File '{filename}' uploaded to '{repo_id}/{path_in_repo}'\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred: {e}\")\n",
        "#     print(\"Please ensure:\")\n",
        "#     print(\"- Your token is valid and has write access.\")\n",
        "#     print(\"- The repository ID and type are correct.\")\n",
        "#     print(\"- The file exists in your Colab environment.\")"
      ],
      "metadata": {
        "id": "2e87rjWjo3Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948b2623-d5c5-4bda-e4fc-f2403f1c9aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'indofashion_results.jsonl' uploaded to 'barryallen16/indofashion-llm-classification/indofashion_results.jsonl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9DHIeqpzwXuD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}